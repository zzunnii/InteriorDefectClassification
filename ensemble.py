#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ì¸í…Œë¦¬ì–´ ê²°í•¨ ë¶„ë¥˜ ëª¨ë¸ ë¶€ìŠ¤íŒ… ì•™ìƒë¸” í‰ê°€
"""

import os
import sys
import argparse
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from collections import defaultdict
from sklearn.metrics import f1_score, confusion_matrix, classification_report

import config
from utils import (
    seed_everything, collect_images, ensure_dir, save_json, load_json,
    get_model_by_name, get_transforms,
    plot_confusion_matrix, plot_class_accuracy, plot_models_comparison
)


def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="ì¸í…Œë¦¬ì–´ ê²°í•¨ ë¶„ë¥˜ ëª¨ë¸ ë¶€ìŠ¤íŒ… ì•™ìƒë¸” í‰ê°€")

    parser.add_argument("--data-dir", type=str, default=config.DATA_DIR,
                        help=f"ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: {config.DATA_DIR})")

    parser.add_argument("--seed", type=int, default=42,
                        help="ë‚œìˆ˜ ì‹œë“œ (ê¸°ë³¸ê°’: 42)")

    parser.add_argument("--optimize-weights", action="store_true", default=True,
                        help="ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜ ê³„ì‚° (ê¸°ë³¸ê°’: True)")

    parser.add_argument("--no-cuda", action="store_true",
                        help="CUDA ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’: False)")

    return parser.parse_args()


class InteriorDataset(torch.utils.data.Dataset):
    """ë¶„ë¥˜ë¥¼ ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹"""

    def __init__(self, paths, labels, transform=None):
        self.paths = paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        path, label = self.paths[idx], self.labels[idx]
        try:
            with open(path, 'rb') as f:
                img = Image.open(f).convert("RGB")
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¡œë”© ì—ëŸ¬ ({path}): {e}")
            img = Image.new("RGB", (256, 256), (0, 0, 0))

        if self.transform:
            img = self.transform(img)

        return img, torch.tensor(label, dtype=torch.long)


@torch.inference_mode()
def predict_with_model(model, data_loader, device):
    """ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰"""
    model.eval()
    all_probs = []
    all_preds = []
    all_labels = []

    for imgs, labels in data_loader:
        imgs = imgs.to(device)
        outputs = model(imgs)
        probs = F.softmax(outputs, dim=1)
        preds = probs.argmax(dim=1)

        all_probs.append(probs.cpu())
        all_preds.extend(preds.cpu().tolist())
        all_labels.extend(labels.tolist())

    return torch.cat(all_probs, dim=0), all_preds, all_labels


def load_models(model_names, num_classes, device):
    """ëª¨ë¸ë“¤ ë¡œë“œ"""
    models = {}
    for model_name in model_names:
        try:
            model_config = config.MODELS[model_name]
            model_path = os.path.join(model_config["result_dir"], "best_model.pth")

            if not os.path.exists(model_path):
                print(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                continue

            model = get_model_by_name(model_name, num_classes).to(device)
            model.load_state_dict(torch.load(model_path, map_location=device))
            models[model_name] = model
            print(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_config['display_name']}")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({model_name}): {e}")

    return models


def calculate_boosting_weights(probs_dict, true_labels, class_names, epsilon=1e-10):
    """AdaBoost ìŠ¤íƒ€ì¼ì˜ ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜ ê³„ì‚°"""
    num_samples = len(true_labels)
    model_names = list(probs_dict.keys())
    num_models = len(model_names)
    num_classes = len(class_names)

    # ì´ˆê¸° ìƒ˜í”Œ ê°€ì¤‘ì¹˜ (ê· ë“± ë¶„í¬)
    sample_weights = np.ones(num_samples) / num_samples

    # ê° ëª¨ë¸ì˜ ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜
    model_weights = {}

    # ëª¨ë¸ ìˆœì„œ ì •ë ¬ (ì„±ëŠ¥ ê¸°ì¤€)
    model_errors = {}
    for model_name in model_names:
        probs = probs_dict[model_name]
        preds = probs.argmax(dim=1).numpy()
        # ì˜¤ë¥˜ìœ¨ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
        incorrect = (preds != np.array(true_labels))
        error_rate = np.sum(sample_weights * incorrect) / np.sum(sample_weights)
        model_errors[model_name] = error_rate

    # ì˜¤ë¥˜ìœ¨ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì •ë ¬ (ì˜¤ë¦„ì°¨ìˆœ)
    sorted_models = sorted(model_errors.items(), key=lambda x: x[1])

    # ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜ ê³„ì‚°
    for model_idx, (model_name, error_rate) in enumerate(sorted_models):
        # ì˜¤ë¥˜ìœ¨ì´ 0ì¸ ê²½ìš° ì²˜ë¦¬ (ì™„ë²½í•œ ëª¨ë¸)
        error_rate = max(error_rate, epsilon)
        error_rate = min(error_rate, 1.0 - epsilon)

        # AdaBoost ê°€ì¤‘ì¹˜ ê³„ì‚°ì‹
        alpha = 0.5 * np.log((1 - error_rate) / error_rate)
        model_weights[model_name] = alpha

        # ë‹¤ìŒ ëª¨ë¸ì„ ìœ„í•œ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        probs = probs_dict[model_name]
        preds = probs.argmax(dim=1).numpy()
        incorrect = (preds != np.array(true_labels))

        # ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (AdaBoost ë°©ì‹)
        sample_weights = sample_weights * np.exp(alpha * incorrect)
        sample_weights = sample_weights / np.sum(sample_weights)

    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
    total_weight = sum(model_weights.values())
    if total_weight > 0:
        for model_name in model_weights:
            model_weights[model_name] /= total_weight
    else:
        # ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ 0ì¸ ê²½ìš° ê· ë“± ë¶„ë°°
        for model_name in model_weights:
            model_weights[model_name] = 1.0 / len(model_weights)

    print("\në¶€ìŠ¤íŒ… ëª¨ë¸ ê°€ì¤‘ì¹˜:")
    for model_name, weight in sorted(model_weights.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {config.MODELS[model_name]['display_name']}: {weight:.4f}")

    return {"boosting": model_weights}


def boosting_ensemble_predict(probs_dict, boosting_weights):
    """ë¶€ìŠ¤íŒ… ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰"""
    model_names = list(probs_dict.keys())
    num_samples = probs_dict[model_names[0]].shape[0]
    num_classes = probs_dict[model_names[0]].shape[1]

    # ê°€ì¤‘ì¹˜ ì•™ìƒë¸” ì˜ˆì¸¡
    weighted_probs = torch.zeros((num_samples, num_classes))

    # ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ ì ìš©
    model_weights = boosting_weights["boosting"]
    for model_name, weight in model_weights.items():
        weighted_probs += weight * probs_dict[model_name]

    # ì˜ˆì¸¡ê°’ ê³„ì‚°
    weighted_preds = weighted_probs.argmax(dim=1).tolist()

    return weighted_probs, weighted_preds


def simple_ensemble_predict(probs_dict):
    """ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸” ì˜ˆì¸¡"""
    all_probs = torch.stack(list(probs_dict.values()))
    mean_probs = all_probs.mean(dim=0)
    preds = mean_probs.argmax(dim=1).tolist()

    return mean_probs, preds


def evaluate_models(models_dict, test_loader, device, class_names):
    """ëª¨ë“  ëª¨ë¸ê³¼ ì•™ìƒë¸” í‰ê°€"""
    results = {}
    probs_dict = {}
    true_labels = None

    # ê°œë³„ ëª¨ë¸ í‰ê°€
    for model_name, model in models_dict.items():
        probs, preds, labels = predict_with_model(model, test_loader, device)

        if true_labels is None:
            true_labels = labels

        acc = (np.array(preds) == np.array(labels)).mean()
        f1 = f1_score(labels, preds, average="weighted")

        display_name = config.MODELS[model_name]["display_name"]
        print(f"{display_name}: Acc={acc:.3f}, F1={f1:.3f}")

        probs_dict[model_name] = probs

        results[model_name] = {
            "display_name": display_name,
            "accuracy": acc,
            "f1_score": f1,
            "predictions": preds
        }

    return results, probs_dict, true_labels


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    args = parse_args()

    # ì‹œë“œ ê³ ì •
    seed_everything(args.seed)

    # CUDA ì„¤ì •
    use_cuda = config.USE_CUDA and torch.cuda.is_available() and not args.no_cuda
    device = torch.device("cuda" if use_cuda else "cpu")
    if use_cuda and config.CUDA_VISIBLE_DEVICES:
        os.environ["CUDA_VISIBLE_DEVICES"] = config.CUDA_VISIBLE_DEVICES

    print(f"Device: {device}")

    # ì•™ìƒë¸” ë””ë ‰í† ë¦¬ ìƒì„±
    ensure_dir(config.ENSEMBLE_DIR)

    # í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ
    train_dir = os.path.join(args.data_dir, "train")
    class_names = sorted(os.listdir(train_dir))
    class_to_idx = {cls: i for i, cls in enumerate(class_names)}
    num_classes = len(class_names)

    print(f"í´ë˜ìŠ¤ ìˆ˜: {num_classes}")

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    test_dir = os.path.join(args.data_dir, "test")
    test_paths, test_labels = collect_images(test_dir, class_to_idx)
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜: {len(test_paths)}")

    # ë°ì´í„° ë¡œë” ìƒì„±
    _, test_transform = get_transforms(img_size=config.TRAIN_CONFIG["image_size"])
    test_dataset = InteriorDataset(test_paths, test_labels, test_transform)
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=config.TRAIN_CONFIG["batch_size"],
        shuffle=False, num_workers=0
    )

    # ëª¨ë¸ ë¡œë“œ
    models = load_models(config.ENSEMBLE_MODELS, num_classes, device)

    if len(models) < 2:
        print("ì•™ìƒë¸”ì„ ìœ„í•œ ëª¨ë¸ì´ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # ëª¨ë¸ë³„ ì„±ëŠ¥ í‰ê°€
    results, probs_dict, true_labels = evaluate_models(models, test_loader, device, class_names)

    # ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜ ê³„ì‚° ë˜ëŠ” ë¡œë“œ
    boosting_weights_path = os.path.join(config.ENSEMBLE_DIR, "boosting_weights.json")

    if args.optimize_weights:
        print("\në¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
        boosting_weights = calculate_boosting_weights(probs_dict, true_labels, class_names)
        save_json(boosting_weights, boosting_weights_path)
        print(f"ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜ ì €ì¥: {boosting_weights_path}")
    elif os.path.exists(boosting_weights_path):
        print(f"ê¸°ì¡´ ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜ ë¡œë“œ: {boosting_weights_path}")
        boosting_weights = load_json(boosting_weights_path)
    else:
        print("ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ì–´ ê· ë“± ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        boosting_weights = {"boosting": {model: 1.0 / len(models) for model in models}}

    # ë¶€ìŠ¤íŒ… ì•™ìƒë¸” ì˜ˆì¸¡
    boosting_probs, boosting_preds = boosting_ensemble_predict(
        probs_dict, boosting_weights
    )

    boosting_acc = (np.array(boosting_preds) == np.array(true_labels)).mean()
    boosting_f1 = f1_score(true_labels, boosting_preds, average="weighted")

    print(f"\nğŸ”¹ ë¶€ìŠ¤íŒ… ì•™ìƒë¸”: Acc={boosting_acc:.3f}, F1={boosting_f1:.3f}")

    # ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸” ì˜ˆì¸¡ (ë¹„êµìš©)
    simple_probs, simple_preds = simple_ensemble_predict(probs_dict)

    simple_acc = (np.array(simple_preds) == np.array(true_labels)).mean()
    simple_f1 = f1_score(true_labels, simple_preds, average="weighted")

    print(f"ğŸ”¹ ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸”: Acc={simple_acc:.3f}, F1={simple_f1:.3f}")

    # ì•™ìƒë¸” ê²°ê³¼ ì¶”ê°€
    results["boosting_ensemble"] = {
        "display_name": "ë¶€ìŠ¤íŒ… ì•™ìƒë¸”",
        "accuracy": boosting_acc,
        "f1_score": boosting_f1,
        "predictions": boosting_preds
    }

    results["simple_ensemble"] = {
        "display_name": "ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸”",
        "accuracy": simple_acc,
        "f1_score": simple_f1,
        "predictions": simple_preds
    }

    # ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥
    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(true_labels, boosting_preds)
    plot_confusion_matrix(
        cm, class_names,
        title="Confusion Matrix - Boosting Ensemble",
        save_path=os.path.join(config.ENSEMBLE_DIR, "confusion_matrix_boosting.png")
    )

    # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    plot_models_comparison(
        {k: {"test_acc": v["accuracy"]} for k, v in results.items()},
        metric="accuracy",
        save_path=os.path.join(config.ENSEMBLE_DIR, "accuracy_comparison_boosting.png")
    )

    plot_models_comparison(
        {k: {"test_f1": v["f1_score"]} for k, v in results.items()},
        metric="f1",
        save_path=os.path.join(config.ENSEMBLE_DIR, "f1_comparison_boosting.png")
    )

    # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
    class_acc = defaultdict(float)
    for true_label, pred_label in zip(true_labels, boosting_preds):
        if true_label == pred_label:
            class_name = class_names[true_label]
            class_acc[class_name] += 1

    for class_name in class_names:
        class_count = sum(1 for label in true_labels if label == class_to_idx[class_name])
        if class_count > 0:
            class_acc[class_name] /= class_count

    # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì‹œê°í™”
    plot_class_accuracy(
        class_acc,
        title="Class-wise Accuracy - Boosting Ensemble",
        save_path=os.path.join(config.ENSEMBLE_DIR, "class_accuracy_boosting.png")
    )

    # í´ë˜ìŠ¤ë³„ ë¶„ë¥˜ ë³´ê³ ì„œ
    classification_rep = classification_report(
        true_labels, boosting_preds,
        target_names=class_names,
        digits=3,
        output_dict=True
    )

    # ëª¨ë“  ê²°ê³¼ ì €ì¥
    final_results = {
        "models": results,
        "boosting_weights": boosting_weights,
        "classification_report": classification_rep,
        "class_accuracy": class_acc
    }

    save_json(final_results, os.path.join(config.ENSEMBLE_DIR, "boosting_results.json"))

    # ì¢…í•© ê²°ê³¼ ì¶œë ¥
    print("\n==== ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ====")
    comparison_df = pd.DataFrame({
        "ëª¨ë¸": [results[m]["display_name"] for m in results.keys()],
        "ì •í™•ë„": [results[m]["accuracy"] for m in results.keys()],
        "F1 ìŠ¤ì½”ì–´": [results[m]["f1_score"] for m in results.keys()]
    })

    print(comparison_df.sort_values("F1 ìŠ¤ì½”ì–´", ascending=False).to_string(index=False))

    # ì„±ëŠ¥ í–¥ìƒ ì •ë„
    best_single_f1 = max([v["f1_score"] for k, v in results.items()
                          if k not in ["boosting_ensemble", "simple_ensemble"]])

    improvement = boosting_f1 - best_single_f1
    print(f"\nğŸ”¹ ìµœê³  ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ: {improvement:.3f} ({improvement * 100:.1f}%)")

    print("\në¶€ìŠ¤íŒ… ì•™ìƒë¸” í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    from PIL import Image

    main()