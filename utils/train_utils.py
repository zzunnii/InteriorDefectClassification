import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from sklearn.metrics import f1_score, confusion_matrix
from tqdm.auto import tqdm

from utils.dataset import mixup_data


def cross_entropy_with_soft_labels(outputs, targets):
    """ì†Œí”„íŠ¸ ë ˆì´ë¸”ì„ ìœ„í•œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜"""
    return -torch.sum(targets * torch.log_softmax(outputs, dim=1), dim=1).mean()


def train_one_epoch(model, loader, optimizer, device, mixup_alpha=0.3, use_soft_labels=False,
                    class_weights_tensor=None):
    """í•œ ì—í­ í•™ìŠµ (ì†Œí”„íŠ¸ ë ˆì´ë¸” + Mixup ì§€ì›)"""
    model.train()
    running_loss, running_correct = 0.0, 0

    for imgs, labels in tqdm(loader, desc="Training", leave=False):
        imgs = imgs.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        # ì†Œí”„íŠ¸ ë ˆì´ë¸” ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬
        if use_soft_labels:
            # Mixup ì ìš© (50% í™•ë¥ )
            if np.random.random() < 0.5 and mixup_alpha > 0:
                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels, mixup_alpha)
                outputs = model(imgs)
                loss = lam * cross_entropy_with_soft_labels(outputs, labels_a) + \
                       (1 - lam) * cross_entropy_with_soft_labels(outputs, labels_b)

                # Mixup ì‚¬ìš© ì‹œ ì •í™•ë„ ê³„ì‚° (ì†Œí”„íŠ¸ ë ˆì´ë¸” ëŒ€ì‘)
                preds = outputs.argmax(1)
                targets_a = labels_a.argmax(1)
                targets_b = labels_b.argmax(1)
                correct = (lam * (preds == targets_a).float() +
                           (1 - lam) * (preds == targets_b).float()).sum().item()
            else:
                outputs = model(imgs)
                loss = cross_entropy_with_soft_labels(outputs, labels)

                # ì •í™•ë„ ê³„ì‚° (ì†Œí”„íŠ¸ ë ˆì´ë¸” ëŒ€ì‘)
                preds = outputs.argmax(1)
                targets = labels.argmax(1)
                correct = (preds == targets).sum().item()

        else:
            # í•˜ë“œ ë ˆì´ë¸” ì‚¬ìš© ì‹œ
            criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)

            # Mixup ì ìš© (50% í™•ë¥ )
            if np.random.random() < 0.5 and mixup_alpha > 0:
                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels, mixup_alpha)
                outputs = model(imgs)
                loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)

                # Mixup ì‚¬ìš© ì‹œ ì •í™•ë„ ê³„ì‚°
                preds = outputs.argmax(1)
                correct = (lam * (preds == labels_a).float() +
                           (1 - lam) * (preds == labels_b).float()).sum().item()
            else:
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                correct = (outputs.argmax(1) == labels).sum().item()

        # ì—­ì „íŒŒ ë° ìµœì í™”
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * imgs.size(0)
        running_correct += correct

    epoch_loss = running_loss / len(loader.dataset)
    epoch_acc = running_correct / len(loader.dataset)

    return epoch_loss, epoch_acc


@torch.inference_mode()
def evaluate(model, loader, criterion, device):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    running_loss, running_correct = 0.0, 0
    preds_all, labels_all = [], []

    for imgs, labels in tqdm(loader, desc="Evaluating", leave=False):
        imgs = imgs.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        outputs = model(imgs)
        loss = criterion(outputs, labels)

        preds = outputs.argmax(1)
        running_loss += loss.item() * imgs.size(0)
        running_correct += (preds == labels).sum().item()

        preds_all.extend(preds.cpu().tolist())
        labels_all.extend(labels.cpu().tolist())

    epoch_loss = running_loss / len(loader.dataset)
    epoch_acc = running_correct / len(loader.dataset)

    # F1 ì ìˆ˜ ê³„ì‚°
    f1 = f1_score(labels_all, preds_all, average="weighted")

    return epoch_loss, epoch_acc, f1, preds_all, labels_all


def train_model(model, train_loader, val_loader, device, config, model_dir,
                use_soft_labels=False, class_weights_tensor=None):
    """ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜"""

    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(model_dir, exist_ok=True)

    # ì†ì‹¤ í•¨ìˆ˜ (ì†Œí”„íŠ¸ ë ˆì´ë¸” ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¦„)
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)

    # ìµœì í™” ì„¤ì •
    optimizer = optim.AdamW(model.parameters(),
                            lr=config['learning_rate'],
                            weight_decay=config['weight_decay'])

    scheduler = ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )

    # í•™ìŠµ ê¸°ë¡
    history = {
        "train_loss": [], "train_acc": [],
        "val_loss": [], "val_acc": [], "val_f1": []
    }

    # ì–¼ë¦¬ ìŠ¤íƒ‘ ì¹´ìš´í„° ë° ìµœê³  ì„±ëŠ¥
    early_stop_counter = 0
    best_val_f1 = 0
    model_path = os.path.join(model_dir, "best_model.pth")

    # í•™ìŠµ ì‹œì‘
    start_time = time.time()

    for epoch in range(1, config['max_epochs'] + 1):
        # í›ˆë ¨
        train_loss, train_acc = train_one_epoch(
            model, train_loader, optimizer, device,
            mixup_alpha=config['mixup_alpha'],
            use_soft_labels=use_soft_labels,
            class_weights_tensor=class_weights_tensor
        )

        # ê²€ì¦
        val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader, criterion, device)

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_loss)

        # ê¸°ë¡ ì €ì¥
        history["train_loss"].append(train_loss)
        history["train_acc"].append(train_acc)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)
        history["val_f1"].append(val_f1)

        # í˜„ì¬ ê²°ê³¼ ì¶œë ¥
        print(f"[{epoch:3d}/{config['max_epochs']}] "
              f"Train L:{train_loss:.4f} A:{train_acc:.3f} | "
              f"Val L:{val_loss:.4f} A:{val_acc:.3f} F1:{val_f1:.3f}")

        # ìµœê³  F1 ìŠ¤ì½”ì–´ ëª¨ë¸ ì €ì¥
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save(model.state_dict(), model_path)
            early_stop_counter = 0
            print(f"ğŸ”¥ New best model saved! (F1: {val_f1:.4f})")
        else:
            early_stop_counter += 1

        # ì–¼ë¦¬ ìŠ¤íƒ‘
        if early_stop_counter >= config['patience']:
            print(f"â›” Early stopping at epoch {epoch}")
            break

    # í•™ìŠµ ì™„ë£Œ
    total_time = time.time() - start_time
    print(f"ì´ í•™ìŠµ ì‹œê°„: {total_time / 60:.2f}ë¶„")

    # ìµœê³  ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(torch.load(model_path))

    return model, history, best_val_f1, total_time


def test_model(model, test_loader, device, class_names, class_weights_tensor=None):
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ í‰ê°€"""
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(
        model, test_loader, criterion, device
    )

    print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: Acc:{test_acc:.3f} F1:{test_f1:.3f}")

    # í˜¼ë™ í–‰ë ¬ ìƒì„±
    cm = confusion_matrix(test_labels, test_preds)

    # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
    class_accuracy = {}
    for i, class_name in enumerate(class_names):
        if np.sum(cm[i]) > 0:
            class_accuracy[class_name] = cm[i, i] / np.sum(cm[i])
        else:
            class_accuracy[class_name] = 0.0

    return {
        "test_acc": test_acc,
        "test_f1": test_f1,
        "confusion_matrix": cm,
        "class_accuracy": class_accuracy,
        "test_preds": test_preds,
        "test_labels": test_labels
    }